{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rajan/Desktop/thesis/thesis_work/minimalist_diffusion'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_ = 0\n",
    "dataset_path = '/home/rajan/Desktop/thesis/thesis_work'\n",
    "model_preloading_path = '/home/rajan/Desktop/thesis/thesis_work/minimalist_diffusion_ckpts/mode_28_ckpt.pth'\n",
    "model_checkpoint_path = f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch_}_ckpt.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimalist_diff_rl_model import minimalDiffRl\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number : 0\n",
      "Average Loss: 1.746806\n",
      "Epoch number : 1\n",
      "Average Loss: 1.566619\n",
      "Epoch number : 2\n",
      "Average Loss: 1.501240\n",
      "Epoch number : 3\n",
      "Average Loss: 1.437793\n",
      "Epoch number : 4\n",
      "Average Loss: 1.416012\n",
      "Epoch number : 5\n",
      "Average Loss: 1.403518\n",
      "Epoch number : 6\n",
      "Average Loss: 1.393356\n",
      "Epoch number : 7\n",
      "Average Loss: 1.385552\n",
      "Epoch number : 8\n",
      "Average Loss: 1.378686\n",
      "Epoch number : 9\n",
      "Average Loss: 1.375112\n",
      "Epoch number : 10\n",
      "Average Loss: 1.369979\n",
      "Epoch number : 11\n",
      "Average Loss: 1.365452\n",
      "Epoch number : 12\n",
      "Average Loss: 1.362093\n",
      "Epoch number : 13\n",
      "Average Loss: 1.358348\n",
      "Epoch number : 14\n",
      "Average Loss: 1.354212\n",
      "Epoch number : 15\n",
      "Average Loss: 1.351843\n",
      "Epoch number : 16\n",
      "Average Loss: 1.348698\n",
      "Epoch number : 17\n",
      "Average Loss: 1.346386\n",
      "Epoch number : 18\n",
      "Average Loss: 1.343361\n",
      "Epoch number : 19\n",
      "Average Loss: 1.340706\n",
      "Epoch number : 20\n",
      "Average Loss: 1.337741\n",
      "Epoch number : 21\n",
      "Average Loss: 1.335737\n",
      "Epoch number : 22\n",
      "Average Loss: 1.333202\n",
      "Epoch number : 23\n",
      "Average Loss: 1.332494\n",
      "Epoch number : 24\n",
      "Average Loss: 1.330571\n",
      "Epoch number : 25\n",
      "Average Loss: 1.328175\n",
      "Epoch number : 26\n",
      "Average Loss: 1.327672\n",
      "Epoch number : 27\n",
      "Average Loss: 1.324922\n",
      "Epoch number : 28\n",
      "Average Loss: 1.325320\n",
      "Epoch number : 29\n",
      "Average Loss: 1.323097\n",
      "Epoch number : 30\n",
      "Average Loss: 1.322335\n",
      "Epoch number : 31\n",
      "Average Loss: 1.321000\n",
      "Epoch number : 32\n",
      "Average Loss: 1.319003\n",
      "Epoch number : 33\n",
      "Average Loss: 1.317857\n",
      "Epoch number : 34\n",
      "Average Loss: 1.316271\n",
      "Epoch number : 35\n",
      "Average Loss: 1.314988\n",
      "Epoch number : 36\n",
      "Average Loss: 1.314375\n",
      "Epoch number : 37\n",
      "Average Loss: 1.313609\n",
      "Epoch number : 38\n",
      "Average Loss: 1.312301\n",
      "Epoch number : 39\n",
      "Average Loss: 1.311053\n",
      "Epoch number : 40\n",
      "Average Loss: 1.309259\n",
      "Epoch number : 41\n",
      "Average Loss: 1.308957\n",
      "Epoch number : 42\n",
      "Average Loss: 1.307309\n",
      "Epoch number : 43\n",
      "Average Loss: 1.307518\n",
      "Epoch number : 44\n",
      "Average Loss: 1.305618\n",
      "Epoch number : 45\n",
      "Average Loss: 1.305817\n",
      "Epoch number : 46\n",
      "Average Loss: 1.305265\n",
      "Epoch number : 47\n",
      "Average Loss: 1.304361\n",
      "Epoch number : 48\n",
      "Average Loss: 1.304154\n",
      "Epoch number : 49\n",
      "Average Loss: 1.303514\n",
      "Epoch number : 50\n",
      "Average Loss: 1.303047\n",
      "Epoch number : 51\n",
      "Average Loss: 1.302800\n",
      "Epoch number : 52\n",
      "Average Loss: 1.302915\n",
      "Epoch number : 53\n",
      "Average Loss: 1.301706\n",
      "Epoch number : 54\n",
      "Average Loss: 1.301472\n",
      "Epoch number : 55\n",
      "Average Loss: 1.300478\n",
      "Epoch number : 56\n",
      "Average Loss: 1.301335\n",
      "Epoch number : 57\n",
      "Average Loss: 1.301126\n",
      "Epoch number : 58\n",
      "Average Loss: 1.300419\n",
      "Epoch number : 59\n",
      "Average Loss: 1.300251\n",
      "Epoch number : 60\n",
      "Average Loss: 1.299292\n",
      "Epoch number : 61\n",
      "Average Loss: 1.299363\n",
      "Epoch number : 62\n",
      "Average Loss: 1.299030\n",
      "Epoch number : 63\n",
      "Average Loss: 1.299176\n",
      "Epoch number : 64\n",
      "Average Loss: 1.298462\n",
      "Epoch number : 65\n",
      "Average Loss: 1.298757\n",
      "Epoch number : 66\n",
      "Average Loss: 1.298367\n",
      "Epoch number : 67\n",
      "Average Loss: 1.298274\n",
      "Epoch number : 68\n",
      "Average Loss: 1.297961\n",
      "Epoch number : 69\n",
      "Average Loss: 1.297975\n",
      "Epoch number : 70\n",
      "Average Loss: 1.297599\n",
      "Epoch number : 71\n",
      "Average Loss: 1.297396\n",
      "Epoch number : 72\n",
      "Average Loss: 1.297389\n",
      "Epoch number : 73\n",
      "Average Loss: 1.297057\n",
      "Epoch number : 74\n",
      "Average Loss: 1.296361\n",
      "Epoch number : 75\n",
      "Average Loss: 1.296390\n",
      "Epoch number : 76\n",
      "Average Loss: 1.296905\n",
      "Epoch number : 77\n",
      "Average Loss: 1.296363\n",
      "Epoch number : 78\n",
      "Average Loss: 1.296110\n",
      "Epoch number : 79\n",
      "Average Loss: 1.296027\n",
      "Epoch number : 80\n",
      "Average Loss: 1.295769\n",
      "Epoch number : 81\n",
      "Average Loss: 1.295981\n",
      "Epoch number : 82\n",
      "Average Loss: 1.295687\n",
      "Epoch number : 83\n",
      "Average Loss: 1.296006\n",
      "Epoch number : 84\n",
      "Average Loss: 1.295260\n",
      "Epoch number : 85\n",
      "Average Loss: 1.295347\n",
      "Epoch number : 86\n",
      "Average Loss: 1.295125\n",
      "Epoch number : 87\n",
      "Average Loss: 1.295582\n",
      "Epoch number : 88\n",
      "Average Loss: 1.295278\n",
      "Epoch number : 89\n",
      "Average Loss: 1.295562\n",
      "Epoch number : 90\n",
      "Average Loss: 1.294591\n",
      "Epoch number : 91\n",
      "Average Loss: 1.294140\n",
      "Epoch number : 92\n",
      "Average Loss: 1.295347\n",
      "Epoch number : 93\n",
      "Average Loss: 1.294326\n",
      "Epoch number : 94\n",
      "Average Loss: 1.294865\n",
      "Epoch number : 95\n",
      "Average Loss: 1.293972\n",
      "Epoch number : 96\n",
      "Average Loss: 1.293720\n",
      "Epoch number : 97\n",
      "Average Loss: 1.293860\n",
      "Epoch number : 98\n",
      "Average Loss: 1.293211\n",
      "Epoch number : 99\n",
      "Average Loss: 1.293690\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Setting up the parameters\n",
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "last_epoch = 0\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "model = torch.nn.DataParallel(minimalDiffRl())\n",
    "model = model.to(device = device)\n",
    "model_last_trained_path = f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{last_epoch}_ckpt.pth'\n",
    "#load the pretrained  DiffusionNet from the checkpoint --> Note that models are being saved in the form of Datparallel objects\n",
    "#so --> need to bring in a new dictionary -> \n",
    "#approach taken from : https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "\"\"\"loading parameters for the diffusion net\"\"\"\n",
    "\n",
    "# ckpt = torch.load(model_preloading_path)\n",
    "# for k, v in ckpt.items():\n",
    "#     name = k[7:] # remove module.\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "# model.module.encoder_drl.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "\"\"\"loading parameters for continual learning\"\"\"\n",
    "# ckpt_last = torch.load(model_last_trained_path)\n",
    "# model.load_state_dict(ckpt_last)\n",
    "\n",
    "    \n",
    "optimizer = Adam(model.parameters(), learning_rate)\n",
    "\n",
    "#defining the loss function \n",
    "def loss_fn(model: minimalDiffRl, x1:torch.tensor, alpha_tensor:torch.tensor):\n",
    "    \"\"\"model : minimalDiffRl object wtih alpha embeddings\n",
    "       x     : input image\n",
    "    \"\"\"\n",
    "    x0 = torch.randn_like(x1).to(device)\n",
    "    alpha = torch.randn(batch_size,).uniform_(0,1).to(device)\n",
    "    blended_x  = (1.-alpha.view(-1,1,1,1))*x0 + alpha.view(-1,1,1,1)*x1\n",
    "    model_output = model(blended_x, alpha)\n",
    "\n",
    "    loss = torch.mean(torch.square(model_output - (x1-x0)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "datasets = MNIST(dataset_path,train=True,transform=transforms.ToTensor(),download=False)\n",
    "data_loader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "t = torch.ones(batch_size).to(device=device)\n",
    "#Training loop \n",
    "for epoch in range(last_epoch, last_epoch+num_epochs):\n",
    "    avg_loss = 0\n",
    "    num_items = 0\n",
    "    for i , (x, y) in enumerate(data_loader):\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        x = x.to(device)\n",
    "        x  = x*2.0 - 1\n",
    "        loss = loss_fn(model,x,t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_items += x.shape[0]\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "    print(f'Epoch number : {epoch}')\n",
    "    print('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "  # Update the checkpoint after each epoch of training.\n",
    "    torch.save(model.state_dict(),f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch}_ckpt.pth')\n",
    "    epoch_ += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "time_steps = 128\n",
    "\n",
    "#define an alpha schedule \n",
    "def alpha_linear_schedule(steps):\n",
    "    \"\"\"steps : number of steps\n",
    "    \"\"\"\n",
    "    return torch.linspace(0,1,steps).to(device=device)\n",
    "\n",
    "def cosine_schedule(alpha_schedule):\n",
    "    \"\"\"alpha_schedule : a linear alpha schedule of t/T where T : max non of steps\n",
    "    \"\"\"\n",
    "    return 1. - torch.cos(alpha_schedule * (torch.pi/2.)).to(device=device)\n",
    "    \n",
    "#for sampling procedure\n",
    "alpha_schedule = alpha_linear_schedule(time_steps)\n",
    "\n",
    "# Need to check the sampling step --> \n",
    "#Sampling steps \n",
    "def sampler(alpha_schedule,model):\n",
    "\n",
    "    x_alpha = torch.rand(32,1,28,28).to(device=device)\n",
    "    t = torch.ones(x_alpha.shape[0]).to(device=device)\n",
    "    for i  in range(1,len(alpha_schedule)):\n",
    "        alpha_tensor = t*alpha_schedule[i-1]\n",
    "        x_alpha = x_alpha + (alpha_schedule[i] - alpha_schedule[i-1])*model(x_alpha,alpha_tensor)\n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "def cosine_function(t: int, T: int):\n",
    "    \"\"\" t: unit time\n",
    "        T: total time steps \n",
    "    \"\"\"\n",
    "    return 1. - torch.cos(torch.tensor((t/T)* (torch.pi/2))).to(device=device)\n",
    " \n",
    "def given_Sampler(model,nb_step):\n",
    "    x_alpha = torch.randn(32,1,28,28).to(device=device)\n",
    "    for t in range(nb_step):\n",
    "        alpha_start = (t/nb_step)\n",
    "        alpha_end =((t+1)/nb_step)\n",
    "\n",
    "        d = model(x_alpha, torch.tensor(alpha_start, device=x_alpha.device))['sample']\n",
    "        x_alpha = x_alpha + (alpha_end-alpha_start)*d\n",
    "\n",
    "    return x_alpha \n",
    "\n",
    "def improved_sampler(model:minimalDiffRl, steps):\n",
    "    \"\"\"model : \n",
    "       steps : total time steps T\n",
    "    \"\"\"\n",
    "    x_alpha = torch.randn(32,1,28,28).to(device=device)\n",
    "    ones = torch.ones(x_alpha.shape[0]).to(device=device)\n",
    "    for t in range(steps):\n",
    "        alpha_half_tensor = ones*cosine_function(t+0.5,steps)\n",
    "        alpha_tensor = ones*cosine_function(t,steps)\n",
    "        x_alpha_half  =  x_alpha + (cosine_function(t+0.5,steps) - cosine_function(t,steps))* model(x_alpha,alpha_tensor)\n",
    "        x_alpha       =  x_alpha + (cosine_function(t+1,steps)   - cosine_function(t,steps))* model(x_alpha_half,alpha_half_tensor)  \n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "#sampling procedure\n",
    "model = minimalDiffRl()\n",
    "\n",
    "\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "\"\"\"loading parameters for the diffusion net\"\"\"\n",
    "\n",
    "\n",
    "ckpt = torch.load('/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_118_ckpt.pth', map_location=device)\n",
    "for k, v in ckpt.items():\n",
    "    name = k[7:] # remove module.\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.to(device=device)\n",
    "model.eval()\n",
    "# samples = sampler(alpha_schedule, model)\n",
    "# samples = given_Sampler(model, nb_step=128)\n",
    "samples = improved_sampler(model,steps=time_steps)\n",
    "\n",
    "\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_grid = make_grid(samples, nrow=int(np.sqrt(32)))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_grid.permute(1,2,0).cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out pixelated Alphas \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimal_diff_rl_model2 import minimalDiffRl_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number : 196\n",
      "Average Loss: 0.405729\n",
      "Epoch number : 197\n",
      "Average Loss: 0.404205\n",
      "Epoch number : 198\n",
      "Average Loss: 0.404395\n",
      "Epoch number : 199\n",
      "Average Loss: 0.404349\n",
      "Epoch number : 200\n",
      "Average Loss: 0.404162\n",
      "Epoch number : 201\n",
      "Average Loss: 0.404181\n",
      "Epoch number : 202\n",
      "Average Loss: 0.404128\n",
      "Epoch number : 203\n",
      "Average Loss: 0.403879\n",
      "Epoch number : 204\n",
      "Average Loss: 0.403803\n",
      "Epoch number : 205\n",
      "Average Loss: 0.403711\n",
      "Epoch number : 206\n",
      "Average Loss: 0.403595\n",
      "Epoch number : 207\n",
      "Average Loss: 0.403536\n",
      "Epoch number : 208\n",
      "Average Loss: 0.403407\n",
      "Epoch number : 209\n",
      "Average Loss: 0.403024\n",
      "Epoch number : 210\n",
      "Average Loss: 0.402953\n",
      "Epoch number : 211\n",
      "Average Loss: 0.403022\n",
      "Epoch number : 212\n",
      "Average Loss: 0.402800\n",
      "Epoch number : 213\n",
      "Average Loss: 0.402473\n",
      "Epoch number : 214\n",
      "Average Loss: 0.402512\n",
      "Epoch number : 215\n",
      "Average Loss: 0.402809\n",
      "Epoch number : 216\n",
      "Average Loss: 0.402355\n",
      "Epoch number : 217\n",
      "Average Loss: 0.402385\n",
      "Epoch number : 218\n",
      "Average Loss: 0.402103\n",
      "Epoch number : 219\n",
      "Average Loss: 0.402151\n",
      "Epoch number : 220\n",
      "Average Loss: 0.401841\n",
      "Epoch number : 221\n",
      "Average Loss: 0.401673\n",
      "Epoch number : 222\n",
      "Average Loss: 0.401632\n",
      "Epoch number : 223\n",
      "Average Loss: 0.401756\n",
      "Epoch number : 224\n",
      "Average Loss: 0.401435\n",
      "Epoch number : 225\n",
      "Average Loss: 0.401370\n",
      "Epoch number : 226\n",
      "Average Loss: 0.401196\n",
      "Epoch number : 227\n",
      "Average Loss: 0.401183\n",
      "Epoch number : 228\n",
      "Average Loss: 0.400855\n",
      "Epoch number : 229\n",
      "Average Loss: 0.400943\n",
      "Epoch number : 230\n",
      "Average Loss: 0.401039\n",
      "Epoch number : 231\n",
      "Average Loss: 0.400677\n",
      "Epoch number : 232\n",
      "Average Loss: 0.400829\n",
      "Epoch number : 233\n",
      "Average Loss: 0.400548\n",
      "Epoch number : 234\n",
      "Average Loss: 0.400231\n",
      "Epoch number : 235\n",
      "Average Loss: 0.400172\n",
      "Epoch number : 236\n",
      "Average Loss: 0.400205\n",
      "Epoch number : 237\n",
      "Average Loss: 0.400241\n",
      "Epoch number : 238\n",
      "Average Loss: 0.400145\n",
      "Epoch number : 239\n",
      "Average Loss: 0.400117\n",
      "Epoch number : 240\n",
      "Average Loss: 0.399557\n",
      "Epoch number : 241\n",
      "Average Loss: 0.399753\n",
      "Epoch number : 242\n",
      "Average Loss: 0.399708\n",
      "Epoch number : 243\n",
      "Average Loss: 0.399114\n",
      "Epoch number : 244\n",
      "Average Loss: 0.399076\n",
      "Epoch number : 245\n",
      "Average Loss: 0.399281\n",
      "Epoch number : 246\n",
      "Average Loss: 0.399023\n",
      "Epoch number : 247\n",
      "Average Loss: 0.398959\n",
      "Epoch number : 248\n",
      "Average Loss: 0.398776\n",
      "Epoch number : 249\n",
      "Average Loss: 0.398424\n",
      "Epoch number : 250\n",
      "Average Loss: 0.398376\n",
      "Epoch number : 251\n",
      "Average Loss: 0.398590\n",
      "Epoch number : 252\n",
      "Average Loss: 0.398374\n",
      "Epoch number : 253\n",
      "Average Loss: 0.398396\n",
      "Epoch number : 254\n",
      "Average Loss: 0.398096\n",
      "Epoch number : 255\n",
      "Average Loss: 0.398034\n",
      "Epoch number : 256\n",
      "Average Loss: 0.397982\n",
      "Epoch number : 257\n",
      "Average Loss: 0.397961\n",
      "Epoch number : 258\n",
      "Average Loss: 0.397457\n",
      "Epoch number : 259\n",
      "Average Loss: 0.397694\n",
      "Epoch number : 260\n",
      "Average Loss: 0.397631\n",
      "Epoch number : 261\n",
      "Average Loss: 0.397428\n",
      "Epoch number : 262\n",
      "Average Loss: 0.396985\n",
      "Epoch number : 263\n",
      "Average Loss: 0.397037\n",
      "Epoch number : 264\n",
      "Average Loss: 0.396898\n",
      "Epoch number : 265\n",
      "Average Loss: 0.396721\n",
      "Epoch number : 266\n",
      "Average Loss: 0.396672\n",
      "Epoch number : 267\n",
      "Average Loss: 0.396405\n",
      "Epoch number : 268\n",
      "Average Loss: 0.396612\n",
      "Epoch number : 269\n",
      "Average Loss: 0.396510\n",
      "Epoch number : 270\n",
      "Average Loss: 0.396144\n",
      "Epoch number : 271\n",
      "Average Loss: 0.396072\n",
      "Epoch number : 272\n",
      "Average Loss: 0.396270\n",
      "Epoch number : 273\n",
      "Average Loss: 0.396156\n",
      "Epoch number : 274\n",
      "Average Loss: 0.395843\n",
      "Epoch number : 275\n",
      "Average Loss: 0.395887\n",
      "Epoch number : 276\n",
      "Average Loss: 0.395979\n",
      "Epoch number : 277\n",
      "Average Loss: 0.395512\n",
      "Epoch number : 278\n",
      "Average Loss: 0.395304\n",
      "Epoch number : 279\n",
      "Average Loss: 0.395537\n",
      "Epoch number : 280\n",
      "Average Loss: 0.395586\n",
      "Epoch number : 281\n",
      "Average Loss: 0.395191\n",
      "Epoch number : 282\n",
      "Average Loss: 0.395090\n",
      "Epoch number : 283\n",
      "Average Loss: 0.395073\n",
      "Epoch number : 284\n",
      "Average Loss: 0.395152\n",
      "Epoch number : 285\n",
      "Average Loss: 0.394804\n",
      "Epoch number : 286\n",
      "Average Loss: 0.394531\n",
      "Epoch number : 287\n",
      "Average Loss: 0.394773\n",
      "Epoch number : 288\n",
      "Average Loss: 0.394539\n",
      "Epoch number : 289\n",
      "Average Loss: 0.394576\n",
      "Epoch number : 290\n",
      "Average Loss: 0.394388\n",
      "Epoch number : 291\n",
      "Average Loss: 0.394137\n",
      "Epoch number : 292\n",
      "Average Loss: 0.393967\n",
      "Epoch number : 293\n",
      "Average Loss: 0.394255\n",
      "Epoch number : 294\n",
      "Average Loss: 0.394110\n",
      "Epoch number : 295\n",
      "Average Loss: 0.393842\n"
     ]
    }
   ],
   "source": [
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "last_epoch = 196\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "model = torch.nn.DataParallel(minimalDiffRl_revised())\n",
    "model = model.to(device = device)\n",
    "model_last_trained_path = f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{last_epoch}_ckpt.pth'\n",
    "#load the pretrained  DiffusionNet from the checkpoint --> Note that models are being saved in the form of Datparallel objects\n",
    "#so --> need to bring in a new dictionary -> \n",
    "#approach taken from : https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "\"\"\"loading parameters for the diffusion net\"\"\"\n",
    "\n",
    "# ckpt = torch.load(model_preloading_path)\n",
    "# for k, v in ckpt.items():\n",
    "#     name = k[7:] # remove module.\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "# model.module.encoder_drl.load_state_dict(new_state_dict)\n",
    "\n",
    "\n",
    "\"\"\"loading parameters for continual learning\"\"\"\n",
    "ckpt_last = torch.load(model_last_trained_path)\n",
    "model.load_state_dict(ckpt_last)\n",
    "\n",
    "    \n",
    "optimizer = Adam(model.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "#defining the loss function \n",
    "def loss_fn(model: minimalDiffRl, x1:torch.tensor):\n",
    "    \"\"\"model : minimalDiffRl object wtih alpha embeddings\n",
    "       x     : input image\n",
    "    \"\"\"\n",
    "    x0 = torch.randn_like(x1).to(device)\n",
    "    alpha = torch.randn_like(x1).uniform_(0,1).to(device)\n",
    "    blended_x  = (1.-alpha)*x0 + alpha*x1\n",
    "    model_output = model(blended_x, alpha)\n",
    "\n",
    "    loss = torch.mean(torch.square(model_output - (x1-x0)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "datasets = MNIST(dataset_path,train=True,transform=transforms.ToTensor(),download=False)\n",
    "data_loader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "\n",
    "datasets = MNIST(dataset_path,train=True,transform=transforms.ToTensor(),download=False)\n",
    "data_loader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "#Training loop \n",
    "for epoch in range(last_epoch, last_epoch+num_epochs):\n",
    "    avg_loss = 0\n",
    "    num_items = 0\n",
    "    for i , (x, y) in enumerate(data_loader):\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        x = x.to(device)\n",
    "        loss = loss_fn(model,x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_items += x.shape[0]\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "    print(f'Epoch number : {epoch}')\n",
    "    print('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "  # Update the checkpoint after each epoch of training.\n",
    "    torch.save(model.state_dict(),f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch}_ckpt.pth')\n",
    "    epoch_ += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sampler_pixel(model:minimalDiffRl_revised,nb_steps=128):\n",
    "    tensor_temp = torch.ones(32,1,28,28).to(device=device)\n",
    "    x_alpha = torch.randn_like(tensor_temp)\n",
    "    for t in range(nb_steps):\n",
    "        alpha_start = t/nb_steps\n",
    "        alpha_end = (t+1)/nb_steps\n",
    "\n",
    "        tensor_temp*= alpha_start\n",
    "\n",
    "        x_alpha = x_alpha + (alpha_end - alpha_start)*model(x_alpha,tensor_temp)\n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "\n",
    "time_steps = 200\n",
    "samples = sampler_pixel(model,nb_steps=time_steps)\n",
    "\n",
    "\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_grid = make_grid(samples, nrow=int(np.sqrt(32)))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_grid.permute(1,2,0).cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_rajan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
