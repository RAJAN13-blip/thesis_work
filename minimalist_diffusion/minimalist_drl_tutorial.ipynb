{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rajan/Desktop/thesis/thesis_work/minimalist_diffusion'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_ = 0\n",
    "dataset_path = '/home/rajan/Desktop/thesis/thesis_work'\n",
    "model_preloading_path = '/home/rajan/Desktop/thesis/thesis_work/minimalist_diffusion_ckpts/mode_28_ckpt.pth'\n",
    "model_checkpoint_path = f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch_}_ckpt.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number : 0\n",
      "Average Loss: 0.430296\n",
      "Epoch number : 1\n",
      "Average Loss: 0.403684\n",
      "Epoch number : 2\n",
      "Average Loss: 0.400908\n",
      "Epoch number : 3\n",
      "Average Loss: 0.399126\n",
      "Epoch number : 4\n",
      "Average Loss: 0.397022\n",
      "Epoch number : 5\n",
      "Average Loss: 0.396239\n",
      "Epoch number : 6\n",
      "Average Loss: 0.395273\n",
      "Epoch number : 7\n",
      "Average Loss: 0.394601\n",
      "Epoch number : 8\n",
      "Average Loss: 0.393324\n",
      "Epoch number : 9\n",
      "Average Loss: 0.392312\n",
      "Epoch number : 10\n",
      "Average Loss: 0.392205\n",
      "Epoch number : 11\n",
      "Average Loss: 0.391652\n",
      "Epoch number : 12\n",
      "Average Loss: 0.390376\n",
      "Epoch number : 13\n",
      "Average Loss: 0.389938\n",
      "Epoch number : 14\n",
      "Average Loss: 0.389627\n",
      "Epoch number : 15\n",
      "Average Loss: 0.389050\n",
      "Epoch number : 16\n",
      "Average Loss: 0.388182\n",
      "Epoch number : 17\n",
      "Average Loss: 0.388080\n",
      "Epoch number : 18\n",
      "Average Loss: 0.386537\n",
      "Epoch number : 19\n",
      "Average Loss: 0.386436\n",
      "Epoch number : 20\n",
      "Average Loss: 0.386506\n",
      "Epoch number : 21\n",
      "Average Loss: 0.386153\n",
      "Epoch number : 22\n",
      "Average Loss: 0.386109\n",
      "Epoch number : 23\n",
      "Average Loss: 0.385381\n",
      "Epoch number : 24\n",
      "Average Loss: 0.384729\n",
      "Epoch number : 25\n",
      "Average Loss: 0.384492\n",
      "Epoch number : 26\n",
      "Average Loss: 0.384537\n",
      "Epoch number : 27\n",
      "Average Loss: 0.384159\n",
      "Epoch number : 28\n",
      "Average Loss: 0.383886\n",
      "Epoch number : 29\n",
      "Average Loss: 0.383703\n",
      "Epoch number : 30\n",
      "Average Loss: 0.383951\n",
      "Epoch number : 31\n",
      "Average Loss: 0.383849\n",
      "Epoch number : 32\n",
      "Average Loss: 0.383387\n",
      "Epoch number : 33\n",
      "Average Loss: 0.383299\n",
      "Epoch number : 34\n",
      "Average Loss: 0.383272\n",
      "Epoch number : 35\n",
      "Average Loss: 0.382287\n",
      "Epoch number : 36\n",
      "Average Loss: 0.383006\n",
      "Epoch number : 37\n",
      "Average Loss: 0.382704\n",
      "Epoch number : 38\n",
      "Average Loss: 0.382838\n",
      "Epoch number : 39\n",
      "Average Loss: 0.382362\n",
      "Epoch number : 40\n",
      "Average Loss: 0.382249\n",
      "Epoch number : 41\n",
      "Average Loss: 0.382340\n",
      "Epoch number : 42\n",
      "Average Loss: 0.382111\n",
      "Epoch number : 43\n",
      "Average Loss: 0.382233\n",
      "Epoch number : 44\n",
      "Average Loss: 0.381662\n",
      "Epoch number : 45\n",
      "Average Loss: 0.382147\n",
      "Epoch number : 46\n",
      "Average Loss: 0.382104\n",
      "Epoch number : 47\n",
      "Average Loss: 0.381966\n",
      "Epoch number : 48\n",
      "Average Loss: 0.381859\n",
      "Epoch number : 49\n",
      "Average Loss: 0.382037\n",
      "Epoch number : 50\n",
      "Average Loss: 0.381476\n",
      "Epoch number : 51\n",
      "Average Loss: 0.381652\n",
      "Epoch number : 52\n",
      "Average Loss: 0.381306\n",
      "Epoch number : 53\n",
      "Average Loss: 0.381501\n",
      "Epoch number : 54\n",
      "Average Loss: 0.381396\n",
      "Epoch number : 55\n",
      "Average Loss: 0.381249\n",
      "Epoch number : 56\n",
      "Average Loss: 0.381244\n",
      "Epoch number : 57\n",
      "Average Loss: 0.381105\n",
      "Epoch number : 58\n",
      "Average Loss: 0.381742\n",
      "Epoch number : 59\n",
      "Average Loss: 0.380481\n",
      "Epoch number : 60\n",
      "Average Loss: 0.381726\n",
      "Epoch number : 61\n",
      "Average Loss: 0.381230\n",
      "Epoch number : 62\n",
      "Average Loss: 0.381396\n",
      "Epoch number : 63\n",
      "Average Loss: 0.381141\n",
      "Epoch number : 64\n",
      "Average Loss: 0.381008\n",
      "Epoch number : 65\n",
      "Average Loss: 0.381263\n",
      "Epoch number : 66\n",
      "Average Loss: 0.381378\n",
      "Epoch number : 67\n",
      "Average Loss: 0.381098\n",
      "Epoch number : 68\n",
      "Average Loss: 0.381319\n",
      "Epoch number : 69\n",
      "Average Loss: 0.381162\n",
      "Epoch number : 70\n",
      "Average Loss: 0.380611\n",
      "Epoch number : 71\n",
      "Average Loss: 0.380869\n",
      "Epoch number : 72\n",
      "Average Loss: 0.380706\n",
      "Epoch number : 73\n",
      "Average Loss: 0.380665\n",
      "Epoch number : 74\n",
      "Average Loss: 0.380679\n",
      "Epoch number : 75\n",
      "Average Loss: 0.380437\n",
      "Epoch number : 76\n",
      "Average Loss: 0.380731\n",
      "Epoch number : 77\n",
      "Average Loss: 0.380543\n",
      "Epoch number : 78\n",
      "Average Loss: 0.380983\n",
      "Epoch number : 79\n",
      "Average Loss: 0.380328\n",
      "Epoch number : 80\n",
      "Average Loss: 0.380459\n",
      "Epoch number : 81\n",
      "Average Loss: 0.380722\n",
      "Epoch number : 82\n",
      "Average Loss: 0.380377\n",
      "Epoch number : 83\n",
      "Average Loss: 0.380439\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 63\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     64\u001b[0m num_items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/thesisnew/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/thesisnew/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesisnew/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/thesisnew/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/thesisnew/lib/python3.10/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from minimalist_diff_rl_model import minimalDiffRl\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "#Setting up the parameters\n",
    "device  = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "model = torch.nn.DataParallel(minimalDiffRl())\n",
    "model = model.to(device = device)\n",
    "\n",
    "#load the pretrained  DiffusionNet from the checkpoint --> Note that models are being saved in the form of Datparallel objects\n",
    "#so --> need to bring in a new dictionary -> \n",
    "#approach taken from : https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "\"\"\"loading parameters for the diffusion net\"\"\"\n",
    "\n",
    "ckpt = torch.load(model_preloading_path)\n",
    "for k, v in ckpt.items():\n",
    "    name = k[7:] # remove module.\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.module.encoder_drl.load_state_dict(new_state_dict)\n",
    "    \n",
    "optimizer = Adam(model.parameters(), learning_rate, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "#defining the loss function \n",
    "def loss_fn(model: minimalDiffRl, x1:torch.tensor, alpha_tensor:torch.tensor):\n",
    "    \"\"\"model : minimalDiffRl object wtih alpha embeddings\n",
    "       x     : input image\n",
    "    \"\"\"\n",
    "    x0 = torch.randn_like(x1).to(device)\n",
    "    alpha = torch.randn(batch_size,).uniform_(0,1).to(device)\n",
    "    blended_x  = (1.-alpha.view(-1,1,1,1))*x0 + alpha.view(-1,1,1,1)*x1\n",
    "    model_output = model(blended_x, alpha)\n",
    "\n",
    "    loss = torch.mean(torch.square(model_output - (x1-x0)))\n",
    "    return loss\n",
    "\n",
    "\n",
    "datasets = MNIST(dataset_path,train=True,transform=transforms.ToTensor(),download=False)\n",
    "data_loader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "t = torch.ones(batch_size).to(device=device)\n",
    "#Training loop \n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    num_items = 0\n",
    "    for i , (x, y) in enumerate(data_loader):\n",
    "        if x.shape[0] != batch_size:\n",
    "            continue\n",
    "        x = x.to(device)\n",
    "        loss = loss_fn(model,x,t)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_items += x.shape[0]\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "    print(f'Epoch number : {epoch}')\n",
    "    print('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "  # Update the checkpoint after each epoch of training.\n",
    "    torch.save(model.state_dict(),f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch}_ckpt.pth')\n",
    "    epoch_ += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "time_steps = 300\n",
    "\n",
    "#define an alpha schedule \n",
    "def alpha_linear_schedule(steps):\n",
    "    \"\"\"steps : number of steps\n",
    "    \"\"\"\n",
    "    return torch.linspace(0,1,steps).to(device=device)\n",
    "\n",
    "def cosine_schedule(alpha_schedule):\n",
    "    \"\"\"alpha_schedule : a linear alpha schedule of t/T where T : max non of steps\n",
    "    \"\"\"\n",
    "    return 1. - torch.cos(alpha_schedule * (torch.pi/2.)).to(device=device)\n",
    "    \n",
    "#for sampling procedure\n",
    "alpha_schedule = alpha_linear_schedule(time_steps)\n",
    "\n",
    "# Need to check the sampling step --> \n",
    "#Sampling steps \n",
    "def sampler(alpha_schedule,model):\n",
    "\n",
    "    x_alpha = torch.randn(32,1,28,28).to(device=device)\n",
    "    for i  in range(1,len(alpha_schedule)):\n",
    "        alpha_tensor = torch.ones(x_alpha.shape[0]).to(device=device)*alpha_schedule[i-1].to(device=device)\n",
    "        x_alpha = x_alpha + (alpha_schedule[i] - alpha_schedule[i-1])*model(x_alpha,alpha_tensor)\n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "def cosine_function(t: int, T: int):\n",
    "    \"\"\" t: unit time\n",
    "        T: total time steps \n",
    "    \"\"\"\n",
    "    return 1. - torch.cos(torch.tensor((t/T)* (torch.pi/2))).to(device=device)\n",
    "\n",
    "\n",
    "def improved_sampler(model:minimalDiffRl, steps):\n",
    "    \"\"\"model : \n",
    "       steps : total time steps T\n",
    "    \"\"\"\n",
    "    x_alpha = torch.randn(32,1,28,28).to(device=device)\n",
    "    ones = torch.ones(x_alpha.shape[0]).to(device=device)\n",
    "    for t in range(steps):\n",
    "        alpha_half_tensor = ones*cosine_function(t+0.5,steps)\n",
    "        alpha_tensor = ones*cosine_function(t,steps)\n",
    "        x_alpha_half  =  x_alpha + (cosine_function(t+0.5,steps) - cosine_function(t,steps))* model(x_alpha,alpha_tensor)\n",
    "        x_alpha       =  x_alpha + (cosine_function(t+1,steps)   - cosine_function(t,steps))* model(x_alpha_half,alpha_half_tensor)  \n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "#sampling procedure\n",
    "model = torch.nn.DataParallel(minimalDiffRl())\n",
    "ckpt = torch.load('/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch}_ckpt.pth', map_location=device)\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "samples = sampler(alpha_schedule, model)\n",
    "\n",
    "# samples = improved_sampler(model,steps=time_steps)\n",
    "\n",
    "\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_grid = make_grid(samples, nrow=int(np.sqrt(32)))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_grid.permute(1,2,0).cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_rajan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
