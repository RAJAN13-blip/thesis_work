{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minimalist_diff_rl_model import minimalDiffRl, device\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#Setting up the parameters\n",
    "batch_size = 32\n",
    "num_epochs = 20 \n",
    "learning_rate = 3e-4\n",
    "model = torch.nn.DataParallel(minimalDiffRl())\n",
    "model = model.to(device = device)\n",
    "\n",
    "#load the pretrained  DiffusionNet from the checkpoint --> Note that models are being saved in the form of Datparallel objects\n",
    "#so --> need to bring in a new dictionary -> \n",
    "#approach taken from : https://discuss.pytorch.org/t/solved-keyerror-unexpected-key-module-encoder-embedding-weight-in-state-dict/1686/4\n",
    "\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "ckpt = torch.load('/home/rajan/Desktop/thesis/thesis_work/minimalist_diffusion_ckpts/mode_28_ckpt.pth')\n",
    "for k, v in ckpt.items():\n",
    "    name = k[7:] # remove module.\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "model.module.encoder_drl.load_state_dict(new_state_dict)\n",
    "    \n",
    "optimizer = Adam(model.parameters(), learning_rate, weight_decay=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "#defining the loss function \n",
    "def loss_fn(model: minimalDiffRl, x1:torch.tensor):\n",
    "    \"\"\"model : minimalDiffRl object wtih alpha embeddings\n",
    "       x     : input image\n",
    "    \"\"\"\n",
    "    x0 = torch.randn_like(x1)\n",
    "    alpha = torch.randn(1,).uniform_(0,1)\n",
    "    alpha_embedding_tensor = alpha*torch.ones(x.shape[0]).to(device=device)\n",
    "    blended_x  = (1.-alpha)*x0 + alpha*x1\n",
    "    model_output = model(blended_x, alpha_embedding_tensor)\n",
    "\n",
    "    loss = torch.mean(torch.square(model_output - (x1-x0)))\n",
    "    return loss\n",
    "\n",
    "datasets = MNIST('/home/rajan/Desktop/thesis/thesis_work',train=True,transform=transforms.ToTensor(),download=False)\n",
    "data_loader = DataLoader(dataset=datasets,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "\n",
    "#Training loop \n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = 0\n",
    "    num_items = 0\n",
    "    for x, y in data_loader:\n",
    "        x = x.to(device)\n",
    "        loss = loss_fn(model,x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_items += x.shape[0]\n",
    "        avg_loss += loss.item() * x.shape[0]\n",
    "    print(f'Epoch number : {epoch}')\n",
    "    print('Average Loss: {:5f}'.format(avg_loss / num_items))\n",
    "  # Update the checkpoint after each epoch of training.\n",
    "    torch.save(model.state_dict(), f'/home/rajan/Desktop/thesis/thesis_work/minimalist_drl_ckpts/model_{epoch}_ckpt.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "\n",
    "time_steps = 300\n",
    "\n",
    "#define an alpha schedule \n",
    "def alpha_linear_schedule(steps):\n",
    "    \"\"\"steps : number of steps\n",
    "    \"\"\"\n",
    "    return torch.linspace(0,1,steps)\n",
    "\n",
    "def cosine_schedule(alpha_schedule):\n",
    "    \"\"\"alpha_schedule : a linear alpha schedule of t/T where T : max non of steps\n",
    "    \"\"\"\n",
    "    return 1. - torch.cos(alpha_schedule * (torch.pi/2.))\n",
    "    \n",
    "#for sampling procedure\n",
    "alpha_schedule = alpha_linear_schedule(time_steps)\n",
    "\n",
    "\n",
    "#Sampling steps \n",
    "def sampler(alpha_schedule,model):\n",
    "\n",
    "    x_alpha = torch.randn(32,1,28,28)\n",
    "    for i  in range(1,len(alpha_schedule)):\n",
    "        alpha_tensor = torch.ones(x_alpha.shape[0])*alpha_schedule[i-1].to(device=device)\n",
    "        x_alpha = x_alpha + (alpha_schedule[i] - alpha_schedule[i-1])*model(x_alpha,alpha_tensor)\n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "def cosine_function(t: int, T: int):\n",
    "    \"\"\" t: unit time\n",
    "        T: total time steps \n",
    "    \"\"\"\n",
    "    return 1. - torch.cos(torch.tensor((t/T)* (torch.pi/2)))\n",
    "\n",
    "\n",
    "def improved_sampler(model:minimalDiffRl, steps):\n",
    "    \"\"\"model : \n",
    "       steps : total time steps T\n",
    "    \"\"\"\n",
    "    x_alpha = torch.randn(32,1,28,28)\n",
    "    for t in range(steps):\n",
    "        alpha_half_tensor = torch.ones(x_alpha.shape[0]).to(device=device)*cosine_function(t+0.5,steps)\n",
    "        alpha_tensor = torch.ones(x_alpha.shape[0]).to(device=device)*cosine_function(t,steps)\n",
    "        x_alpha_half  =  x_alpha + (cosine_function(t+0.5,steps) - cosine_function(t,steps))* model(x_alpha,alpha_tensor)\n",
    "        x_alpha       =  x_alpha + (cosine_function(t+1,steps)   - cosine_function(t,steps))* model(x_alpha_half,alpha_half_tensor)  \n",
    "\n",
    "    return x_alpha\n",
    "\n",
    "#sampling procedure\n",
    "model = torch.nn.DataParallel(minimalDiffRl())\n",
    "ckpt = torch.load('./minimalist_drl_ckpts/model_18_ckpt.pth', map_location=device)\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "samples = sampler(alpha_schedule, model)\n",
    "\n",
    "\n",
    "samples = samples.clamp(0.0, 1.0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_grid = make_grid(samples, nrow=int(np.sqrt(32)))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.axis('off')\n",
    "plt.imshow(sample_grid.permute(1,2,0).cpu(), vmin=0., vmax=1.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_rajan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
